<html>
<head>
    <link rel="Stylesheet" type="text/css" href="style.css" />
    <title>Linear Algebra</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/gh/mathjax/MathJax@2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
    <a href="index.html">Index</a>
    <hr>
    <div class="content">
    
<div id="Linear Algebra"><h1 id="Linear Algebra">Linear Algebra</h1></div>
<div id="Contents" class="toc"><h1 id="Contents">Contents</h2></div>
<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Introduction">Introduction</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Introduction-Linear Equations">Linear Equations</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Introduction-Matrix notation">Matrix notation</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Introduction-Reducing a matrix">Reducing a matrix</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Introduction-Vectors">Vectors</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Solution sets of linear systems">Solution sets of linear systems</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Solution sets of linear systems-Homogeneous linear systems">Homogeneous linear systems</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Solution sets of linear systems-Parametric vector form">Parametric vector form</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Solution sets of linear systems-Linear independence">Linear independence</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Linear transformations">Linear transformations</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations">Matrix operations</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations-Sums and scalar multiples">Sums and scalar multiples</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations-Powers of a matrix">Powers of a matrix</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations-Transpose of a matrix">Transpose of a matrix</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations-Inverse of a matrix">Inverse of a matrix</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Matrix operations-Elementary matrices">Elementary matrices</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics">Applications to computer graphics</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics-Homogeneous coordinates">Homogeneous coordinates</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics-Homogeneous coordinates-2D">2D</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics-Homogeneous coordinates-3D">3D</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics-Composite transformations">Composite transformations</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Applications to computer graphics-Perspective projections">Perspective projections</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Subspaces">Subspaces</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Subspaces-Column space and null space of a matrix">Column space and null space of a matrix</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Subspaces-Basis for a subspace">Basis for a subspace</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Subspaces-Coordinates">Coordinates</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Subspaces-Dimension of a subspace">Dimension of a subspace</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Eigenvectors &amp; eigenvalues">Eigenvectors &amp; eigenvalues</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Determinant">Determinant</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Similarity">Similarity</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Diagonalization">Diagonalization</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Inner (dot) product &amp; uses">Inner (dot) product &amp; uses</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Inner (dot) product &amp; uses-Length of a vector">Length of a vector</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Inner (dot) product &amp; uses-Distance between vectors">Distance between vectors</a>

</ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Orthogonality">Orthogonality</a>

<ul>
<li>
<a href="Linear Algebra.html#Linear Algebra-Orthogonality-Orthogonal complement">Orthogonal complement</a>

<li>
<a href="Linear Algebra.html#Linear Algebra-Orthogonality-Orthogonal sets">Orthogonal sets</a>

</ul>
</ul>

<div id="Linear Algebra-Introduction"><h2 id="Introduction">Introduction</h2></div>
<div id="Linear Algebra-Introduction-Linear Equations"><h3 id="Linear Equations">Linear Equations</h3></div>
<p>
"the study of linear equations"
</p>

<p>
a linear equation in the variables \(x_1, \dots, x_n\) has the form \(a_1 x_1+\dots+a_n x_n = b\), with \(a_1, \dots, a_n\) being the <em>coefficients</em>
</p>

<p>
geometric interpretation:
</p>

\[
\begin{alignat*}{3}
&amp;n=1\qquad &amp;&amp;a_1 x_1 = b \longrightarrow x_1 = \frac{b}{a_1}\qquad &amp;&amp;\text{(point on a line in $\Re$)}\\
&amp;n=2\qquad &amp;&amp;a_1 x_1 + a_2 x_2 = b \longrightarrow x_2 = \frac{b}{a_2} - \frac{a_1}{a_2}\qquad &amp;&amp;\text{(line in a plane in $\Re^2$)}\\
&amp;n=3\qquad &amp;&amp;a_1 x_1 + a_2 x_2 + a_3 x_3 = b\qquad &amp;&amp;\text{(planes in 3D space, in $\Re^3$)}
\end{alignat*}
\]

<p>
in general, \(n-1\)-dimensional planes in n-dimensional space
</p>

<p>
system of linear equations \(x_1, \dots, x_n\) is a collection of linear equations in these variables.
</p>

<p>
\(x_1 - 2x_2 = -1\)
</p>

<p>
\(-x_1 + 3x_2 = 3\)
</p>

<p>
If you graph them, you get this:
</p>

<p>
<img src="img-algebra/graph-example.png" alt="System of equations graph" />
</p>

<p>
the solution is the intersection.
</p>

<p>
a system of linear equations has:
</p>
<ol>
<li>
no solutions (inconsistent) -- e.g. parallel lines

<li>
exactly 1 solution (consistent)

<li>
infinitely many solutions (consistent) - e.g. the same line twice

</ol>

<p>
two linear systems are "equivalent" if they have the same solutions.
</p>

<div id="Linear Algebra-Introduction-Matrix notation"><h3 id="Matrix notation">Matrix notation</h3></div>
<table>
<tr>
<th>
Equation
</th>
<th>
(Augmented) coefficient matrix notation
</th>
</tr>
<tr>
<td>
\(\begin{alignat*}{6} &amp;x_1 &amp;&amp;-&amp;&amp;2x_2 &amp;&amp;+&amp;&amp;x_3 &amp;&amp;= 0\\ &amp; &amp;&amp; &amp;&amp;2x_2 &amp;&amp;-&amp;&amp;8x_3 &amp;&amp;= 8\\ &amp;5x_1 &amp;&amp; &amp;&amp; &amp;&amp;-&amp;&amp;5x_3 &amp;&amp;= 10\end{alignat*}\)
</td>
<td>
\(\begin{bmatrix} 1 &amp; -2 &amp; 1 &amp; 0\\ 0 &amp; 2 &amp; -8 &amp; 8\\ 5 &amp; 0 &amp; -5 &amp; 10 \end{bmatrix}\)
</td>
</tr>
</table>

<p>
the strategy to solve is to replace the system with an equivalent system that is easier to solve.
</p>

<p>
elementary row operations:
</p>
<ol>
<li>
replacement: add rows

<li>
scaling: multiply by constant (non-zero scalar)

<li>
interchange: swap two rows

</ol>

<p>
all of these are reversible &amp; don't change the solution set.
</p>

<p>
Matrices A and B are equivalent (\(A \sim B\)) if there is a sequence of elementary operations to transform A to B.
</p>

<p>
If augmented matrices of two systems are row-equivalent, then the systems are equivalent.
</p>

<p>
Matrix A is in echelon form if:
</p>
<ol>
<li>
zero rows are below non-zero rows

<li>
the leading entry of a row is contained in a column that is to the left of the leading entry of the row below it.

</ol>

<p>
A is in reduced echelon form if:
</p>
<ol>
<li>
A is in echelon form

<li>
all leading entries are 1

<li>
the leading entry is the only non-zero entry in that column

</ol>

<div id="Linear Algebra-Introduction-Reducing a matrix"><h3 id="Reducing a matrix">Reducing a matrix</h3></div>
<p>
The reduced echelon form of a matrix is unique.
</p>

<p>
every matrix is row-equivalent to a unique reduced echelon matrix.
</p>

<p>
the positions of the leading entries in an echelon matrix are unique
</p>

<p>
\(\begin{bmatrix} \textbf{1} &amp; * &amp; * &amp; *\\ 0 &amp; 0 &amp; \textbf{1} &amp; *\\ 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\)
</p>

<p>
the values in bold are pivot positions. the columns containing those values are pivot columns.
</p>

<p>
Row reduction algorithm:
</p>
<ol>
<li>
Start with leftmost non-zero column (pivot column)

<li>
Select a non-zero entry as pivot and move it to the pivot position.

<li>
Create zeros below the pivot position.

<li>
Ignore the row containing the pivot position &amp; repeat steps 1-3 until solved. The matrix will be in echelon form.

<li>
Make pivot positions equal to 1, create zeros in all pivot columns. Start with the rightmost column. The matrix will be in reduced echelon form.

</ol>

<p>
Side note: a computer chooses as pivot the entry that's smallest in absolute value to minimize the round-off error.
</p>

<p>
Basic variables correspond to pivot columns. Free variables correspond to non-pivot columns. You solve the equation by expressing basic variables in terms of free variables.
</p>

<p>
The matrix can be written in parametric form, example with \(x_3\) being a free variable:
</p>

<p>
\(\binom{x_1}{x_2} = \big\{ \binom{1}{4} + \binom{5}{-1} x_3 \;\rvert\; x_3 \in \Re \big\}\)
</p>

<p>
if there are any free variables, there are infinite solutions.
</p>

<div id="Linear Algebra-Introduction-Vectors"><h3 id="Vectors">Vectors</h3></div>
<p>
A vector is a line. If you have a vector in the form \(\begin{bmatrix} a\\b\end{bmatrix}\), you can draw it as an arrow from the origin ending at the point \((a,b)\).
</p>

<p>
To add vectors, add the individual cells together.
</p>

<p>
A vector equation \(a_1 x_1 + a_2 x_2 + \dots + a_n x_n = b\) has the same solution set as \(\begin{bmatrix} a_1 &amp; a_2 &amp; \dots &amp; a_n &amp; b \end{bmatrix}\).
</p>

<p>
When asked whether \(b\) is in \(\text{Span} \{v_1, \dots, v_p\}\), you have to check whether the augmented matrix \(\begin{bmatrix} v_1 &amp; \dots &amp; v_p &amp; b \end{bmatrix}\) has a solution.
</p>

<p>
\(b\) is a linear combination of \(A\) if \(Ax = b\) has a solution.
</p>

<p>
The span is the set of all linear combinations of the vectors.
</p>

<p>
To calculate \(Ax\), if the number of columns in A is the same as the number of rows in x, you can follow the definition:
</p>

\[
Ax = \begin{bmatrix} a_1 &amp; a_2 &amp; \dots &amp; a_n \end{bmatrix} \begin{bmatrix} x_1 \\ \dots \\ x_n \end{bmatrix} = x_1 a_1 + x_2 a_2 + \dots + x_n a_n
\]

<p>
You also have the rules (matrix A,  vectors u and v, scalar c):
</p>
<ul>
<li>
\(A(u+v) = Au + Av\)

<li>
\(A(cu) = c(Au)\)

</ul>

<div id="Linear Algebra-Solution sets of linear systems"><h2 id="Solution sets of linear systems">Solution sets of linear systems</h2></div>
<div id="Linear Algebra-Solution sets of linear systems-Homogeneous linear systems"><h3 id="Homogeneous linear systems">Homogeneous linear systems</h3></div>
<p>
homogeneous: if you can write it in \(Ax = 0\) where A is an \(m \times n\) matrix and 0 is the zero vector in \(\Re^m\)
</p>
<ul>
<li>
always has at least one solution (the trivial solution, \(x = 0\)).

<li>
has a nontrivial solution iff there is a free variable

<ul>
<li>
if the equation has only one free variable, the solution is a line through the origin

<li>
when there are two or more free variables, it's a line through the origin

</ul>
<li>
solution set is \(\text{Span} \{v_1, \ldots, v_p\}\) for suitable vectors

</ul>

<div id="Linear Algebra-Solution sets of linear systems-Parametric vector form"><h3 id="Parametric vector form">Parametric vector form</h3></div>
<p>
implicit description:
</p>
<ul>
<li>
a simple equation

<li>
e.g. \(10x_1 - 3x_2 - 2x_3 = 0\)

</ul>

<p>
explicit description (parametric vector form):
</p>
<ul>
<li>
the solution to the equation as a set spanned by u and v

<li>
of the form \(x = su + tv\), with \(s,t \in \Re\)

</ul>

<p>
the solution set of \(Ax = 0\) is \(x = tv\) with \(t \in \Re\).
</p>

<p>
if \(Ax = b\) has a solution, then you get the solution set by translating the solution set of \(Ax = 0\) using any particular solution p of \(Ax = b\). The set is then \(x = p + tv\)
</p>

<p>
Writing a solution set in parametric vector form:
</p>
<ol>
<li>
Row reduce augmented matrix to echelon form

<li>
Express each basic variable in terms of any free variables.

<li>
Write a typical solution x as a vector, with entries depending on the (potential) free variables.

<li>
Decompose x into a linear combination of vectors using free vars as parameters.

</ol>

<div id="Linear Algebra-Solution sets of linear systems-Linear independence"><h3 id="Linear independence">Linear independence</h3></div>

<p>
linearly independent:
</p>
<ul>
<li>
set of vector equations: iff the vector equation has only the trivial solution (\(x_1 = x_2 = x_3 = 0\))

<li>
columns of matrix: iff \(Ax = 0\) has <em>only</em> the trivial solution

<li>
one vector: iff v is not the zero vector

<li>
two vectors: if neither of the vectors is a multiple of the other

</ul>

<p>
linearly dependent:
</p>
<ul>
<li>
iff at least one of the vectors is a linear combination of the others

<li>
if there are more vectors than entries in each vector

<li>
if the set contains the zero vector

</ul>

<p>
a set is linearly dependent iff it's not linearly independent.
</p>

<div id="Linear Algebra-Linear transformations"><h2 id="Linear transformations">Linear transformations</h2></div>
<p>
definitions:
</p>
<ul>
<li>
transformation, function, mapping: rule assigning to each vector in \(\Re^n\) a vector \(T(x)\) in \(\Re^m\)

<li>
domain: set \(\Re^n\)

<li>
codomain: set \(\Re^m\)

<li>
image: vector T(x)

<li>
range: set of all images T(x)

</ul>

<p>
a projection transformation happens if you go to a lower dimension (e.g. \(x_3\) becomes 0). a shear transformation happens if a 2D square is tilted sideways into a parallelogram.
</p>

<p>
a transformation T is linear if:
</p>
<ol>
<li>
\(T(u + v) = T(u) + T(v)\) for all \(u,v \in \text{Domain}(T)\)

<li>
\(T(cu) = cT(u)\) for all scalars c and all \(u \in \text{Domain}(T)\)

</ol>

<p>
linear transformations preserve operations of vector addition and scalar multiplication.
</p>

<p>
if T is a linear transformation, then:
</p>
<ul>
<li>
\(T(0) = 0)\)

<li>
\(T(cu + dv) = cT(u) + dT(v)\)

<li>
\(T(c_1 v_2 + \dots + c_p v_p) = c_1 T(v_1) + \dots + c_p T(v_p)\) (superposition principle)

</ul>

<p>
given scalar r, and \(T: \Re^2 \rightarrow \Re^2\) by \(T(x) = rx\)
</p>
<ul>
<li>
contraction: when \(0 \leq r \leq 1\)

<li>
dilation: when \(r &gt; 1\)

</ul>

<p>
every linear transformation \(\Re^n \rightarrow \Re^m\) is a matrix transformation \(x \mapsto Ax\). 
</p>

<p>
\(A = [[T(e_1) \dots T(e_n)]\), where \(e_j\) is the jth column of the identity matrix in \(\Re^n\)
</p>

<p>
geometric linear transformations of \(\Re^2\):
</p>

<p>
<img src="img-algebra/geo-reflections.png" alt="Reflections" /> <img src="img-algebra/geo-contract-shears.png" alt="Contractions/expansions and shears" /> <img src="img-algebra/geo-projections.png" alt="Projections" />
</p>

<p>
types of mappings:
</p>
<ul>
<li>
\(T: \Re^n \rightarrow \Re^m\) is 'onto' \(\Re^m\) if <em>each</em> b in \(\Re^m\) is the image of <em>at least one</em> x in \(\Re^n\).

<li>
\(T: \Re^n \rightarrow \Re^m\) is one-to-one if <em>each</em> b in \(\Re^m\) is the image of <em>max one</em> x in \(\Re^n\).

<ul>
<li>
so if \(T(x) = 0\) only has the trivial solution

</ul>
</ul>

<p>
for mapping \(T: \Re^n \rightarrow \Re^m\) and standard matrix \(A\):
</p>
<ul>
<li>
T maps \(\Re^n\) onto \(\Re^m\) iff columns of matrix span \(\Re^m\)

<li>
T is one-to-one iff columns of matrix are linearly independent.

</ul>

<div id="Linear Algebra-Matrix operations"><h2 id="Matrix operations">Matrix operations</h2></div>
<p>
\(a_{ij}\) is the entry in the ith row and jth column of A
</p>

<p>
diagonal entries are \(a_{11}\), \(a_{22}\), etc. and form the main diagonal. if non-diagonal entries are zero, then it's a diagonal matrix.
</p>

<p>
equal matrices have same size <em>and</em> their corresponding entries are equal.
</p>

<div id="Linear Algebra-Matrix operations-Sums and scalar multiples"><h3 id="Sums and scalar multiples">Sums and scalar multiples</h3></div>
<p>
sum A+B: sum corresponding entries in A and B.
</p>

<p>
scalar multiple \(rA\) is matrix whose columns are r times the corresponding columns in A (with r scalar).
</p>

<p>
the usual rules of algebra apply to sums and scalar multiples of matrices.
</p>

<p>
when matrix B multiplies vector x, it transforms x into vector \(Bx\). if \(Bx\) is multiplied by A, the result is \(A(Bx)\). \(A(Bx)\) is produced from x by composition of mappings, which can be represented as multiplication by a single matrix AB.
</p>

<p>
\(A(Bx) = (AB)x\)
</p>

<p>
\(AB = A \begin{bmatrix} b_1 &amp; b_2 &amp; \dots &amp; b_p \end{bmatrix} = \begin{bmatrix} Ab_1 &amp; Ab_2 &amp; \dots &amp; Ab_p \end{bmatrix}\)
</p>

<p>
A is matrix, B is matrix with columns \(b_1 \dots b_p\).
</p>

<p>
each column of AB is a linear combination of columns of A using weights from corresponding column of B. AB has the same number of rows as A and same number of columns as B. if the number of columns of A does not match number of rows of B, the product is undefined. in general, AB ≠ BA.
</p>

<p>
if product AB is defined, then:
</p>


<p>
\((AB)_{ij} = a_{i1} b_{1j} + a_{i2} b_{2j} + \dots + a_{in} b_{nj}\)
</p>

<p>
\(row_i (AB) = row_i (A) \times B\)
</p>

<div id="Linear Algebra-Matrix operations-Powers of a matrix"><h3 id="Powers of a matrix">Powers of a matrix</h3></div>
<p>
\(A^k = \underbrace{A \dots A}_{k}\)
</p>

<p>
with \(A\) an n × n matrix and k a positive integer.
</p>

<div id="Linear Algebra-Matrix operations-Transpose of a matrix"><h3 id="Transpose of a matrix">Transpose of a matrix</h3></div>
<p>
a matrix \(A'\) whose columns are made up of the corresponding rows of \(A\)
</p>

<p>
properties:
</p>
<ul>
<li>
\((A^T)^T = A\)

<li>
\((A+B)^T = A^T + B^T\)

<li>
\((rA)^T = rA^T\) with r a scalar

<li>
\((AB)^T = B^T A^T\)$

</ul>

<p>
the transpose of a product of matrices == product of their transposes in reverse order
</p>

<div id="Linear Algebra-Matrix operations-Inverse of a matrix"><h3 id="Inverse of a matrix">Inverse of a matrix</h3></div>
<p>
invertible (singular) if there is same size matrix C such that \(CA = I\) and \(AC = I\) where I is the n × n identity matrix.
</p>

<p>
identity matrix: a matrix where the diagonals are all 1.
</p>

<p>
C is uniquely determined by A, so: \(A^{-1} A = I\).
</p>

<p>
let \(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}.\) if \(ad - bc \ne 0\) then \(A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}\)
</p>

<p>
determinant: \(\det A = ad - bc\)
</p>

<p>
if A is invertible (determinant is not 0), then for each \(b \in \Re^n\) the solution of \(Ax = b\) is \(A^{-1} b\).
</p>

<p>
properties of inverse:
</p>
<ul>
<li>
\((A^{-1})^{-1} = A\)

<li>
\((AB)^{-1} = B^{-1} A^{-1}\) (watch out for order!)

<li>
\((A^T)^{-1} = (A^{-1})^T\)

</ul>

<p>
finding \(A^{-1}\):
</p>
<ul>
<li>
Row reduce augmented matrix \(\begin{bmatrix} A &amp; I \end{bmatrix}\).

<li>
if A is row equivalent to I, then \(\begin{bmatrix} A &amp; I \end{bmatrix}\) is row equivalent to \(\begin{bmatrix} I &amp; A^{-1} \end{bmatrix}\)

<li>
otherwise, A doesn't have an inverse.

</ul>

<div id="Linear Algebra-Matrix operations-Elementary matrices"><h3 id="Elementary matrices">Elementary matrices</h3></div>
<p>
elementary matrix: obtained by performing single elementary row operation on identity matrix
</p>

<p>
if elementary row operation is performed on m × n matrix A, result is EA, with E an m × m matrix obtained by performing same row operation on \(I_m\)
</p>

<p>
inverse of any elementary matrix E is of same type that transforms E back into I.
</p>

<p>
an n × n matrix A is only invertible if A is row equivalent to \(I_n\). any sequence of elementary operations reducing A to \(I_n\) also transforms \(I_n\) into \(A^{-1}\).
</p>

<div id="Linear Algebra-Applications to computer graphics"><h2 id="Applications to computer graphics">Applications to computer graphics</h2></div>
<p>
graphics are stored in a matrix, such as this:
</p>

<p>
<img src="img-algebra/graphics-coordinate-matrix.png" alt="Graphics coordinate matrix" /> <img src="img-algebra/vector-letter-n.png" alt="Vector letter N" />
</p>

<div id="Linear Algebra-Applications to computer graphics-Homogeneous coordinates"><h3 id="Homogeneous coordinates">Homogeneous coordinates</h3></div>
<div id="Linear Algebra-Applications to computer graphics-Homogeneous coordinates-2D"><h4 id="2D">2D</h4></div>
<p>
each point (x, y) in 2D can be identified with point (x, y, 1) in 3D. so we say that (x, y) has homogeneous coordinates (x, y, 1).
</p>

<p>
e.g. translation is not a linear transformation. but \((x, y) \mapsto (x+h, y+k)\) can be written in homogeneous coordinates as \((x, y, 1) \mapsto (x+h, y+k, 1)\), and can be computed using matrix multiplication:
</p>

<p>
\(\begin{bmatrix} 1 &amp; 0 &amp; h\\ 0 &amp; 1 &amp; k\\ 0 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} = \begin{bmatrix} x+h \\ y+k \\ 1 \end{bmatrix}\)
</p>

<div id="Linear Algebra-Applications to computer graphics-Homogeneous coordinates-3D"><h4 id="3D">3D</h4></div>
<p>
(X, Y, Z, H) are homogeneous coordinates for (x, y, z) if H ≠ 0 and
</p>

<p>
\(x = \frac{X}{H}, \quad y = \frac{Y}{H}, \quad \text{and} \; z = \frac{Z}{H}\)
</p>

<div id="Linear Algebra-Applications to computer graphics-Composite transformations"><h3 id="Composite transformations">Composite transformations</h3></div>
<p>
when you need two or more basic transformations, such a composite transformation is a matrix multiplication.
</p>

<div id="Linear Algebra-Applications to computer graphics-Perspective projections"><h3 id="Perspective projections">Perspective projections</h3></div>
<p>
maps each point (x, y, z) onto an image point (x*, y*, 0) so that two points and eye position (center of projection) are on a line.
</p>

<p>
<img src="img-algebra/perspective-projection-diagram.png" alt="Perspective projection diagram" />
</p>

<div id="Linear Algebra-Subspaces"><h2 id="Subspaces">Subspaces</h2></div>
<p>
subspace of \(\Re^n\) is any set H in \(\Re^n\) that has properties:
</p>
<ol>
<li>
The zero vector is in H

<li>
For each u and v in H, the sum \(u + v\) is in H

<li>
For each u in H and each scalar c, the vector \(cu\) is in H

</ol>

<p>
the zero subspace is the set that only contains zero vector in \(\Re^n\)
</p>

<div id="Linear Algebra-Subspaces-Column space and null space of a matrix"><h3 id="Column space and null space of a matrix">Column space and null space of a matrix</h3></div>
<p>
column space: set of all linear combinations of the columns of a matrix
</p>

<p>
column space of m × n matrix is subspace of \(\Re^m\)
</p>

<p>
null space: set of all solutions of equation \(Ax = 0\).
</p>

<p>
null space of an m × n matrix is subspace of \(\Re^n\).
</p>

<div id="Linear Algebra-Subspaces-Basis for a subspace"><h3 id="Basis for a subspace">Basis for a subspace</h3></div>
<p>
basis for subspace H of \(\Re^n\) is linearly independent set in H spanning H
</p>

<p>
the pivot columns of a matrix form the basis for its column space.
</p>

<div id="Linear Algebra-Subspaces-Coordinates"><h3 id="Coordinates">Coordinates</h3></div>
<p>
let \(H \in \Re^n\) be subspace with \(B = \{ b_1, \dots, b_p\}\). then for all x ∈ H, there are unique \(c_1, \dots, c_p\) such that \(x = c_1 b_2 + \dots + c_p b_p\). (to prove this theorem, use a contradiction on uniqueness)
</p>

<p>
the coordinates of x w.r.t. B are \(c_1, \dots, c_p\).
</p>

<p>
the coordinate system of x w.r.t. B is \([x]_B = \begin{bmatrix}c_1\\ \dots\\ c_p\end{bmatrix}\)
</p>

<div id="Linear Algebra-Subspaces-Dimension of a subspace"><h3 id="Dimension of a subspace">Dimension of a subspace</h3></div>
<p>
let \(H \in \Re^n\) be a subspace with basis \(B=\{ b_1, \dots, b_p \}\). then every basis for H comprises p vectors.
</p>

<p>
the dimension of H is the number of basis vectors in <em>any</em> basis for H.
</p>

<p>
dim Col A = #pivot columns (rank A)
</p>

<p>
dim Nul A = #free variables in Ax = 0
</p>

<p>
Rank theorem: dim Col A + dim Nul A = #columns
</p>

<div id="Linear Algebra-Eigenvectors &amp; eigenvalues"><h2 id="Eigenvectors &amp; eigenvalues">Eigenvectors &amp; eigenvalues</h2></div>
<p>
let A be n × n, \(x \in \Re^n\) is an eigenvector of A if x ≠ 0 and \(\exists \lambda \in \Re\) such that \(Ax = \lambda x\) 
</p>

<p>
x is eigenvector with corresponding eigenvalue λ.
</p>

<p>
Is a given vector \(u \in \Re^n\) an eigenvector of a given A (n × n)?
</p>
<ul>
<li>
Do \(Au\), check if result is a multiple of u.

</ul>

<p>
Is a given λ an eigenvalue of A?
</p>
<ul>
<li>
\(\exists x \ne 0\) such that \(Ax - \lambda x = 0 \leftrightarrow (A-\lambda I_n)x = 0\) with nontrivial solutions.

</ul>

<p>
The solution set of \((A-\lambda I_n)x = 0\) is the eigenspace corresponding to λ.
</p>

<p>
How to find a basis for the eigenspace of a given λ?
</p>
<ol>
<li>
calculate matrix for \(A-\lambda I_n\) where n is the number of rows or columns of A

<li>
reduce matrix to reduced echelon form

<li>
express solutions in parametric form (basic variables in terms of free variables)

<li>
basis for eigenspace is the set of the coefficients

</ol>

<p>
If λ = 0, then Ax = 0 has a nontrivial solution (and A is <em>not</em> invertible).
</p>

<p>
Eigenvectors corresponding to distinct eigenvalues are linearly independent.
</p>

<div id="Linear Algebra-Determinant"><h2 id="Determinant">Determinant</h2></div>
<p>
Geometric interpretation: let \(A = [a_1 \; a_2]\). then the determinant (absolute value) is the surface area (or volume in 3D):
</p>

<p>
<img src="img-algebra/determinant-geometric-diagram.png" alt="Determinant geometric diagram" />
</p>

<p>
Let A (n × n). A ~ U without scaling and using <em>r</em> row interchanges. then \(\det A = (-1)^r u_{11} \times \dots \times u_{nn}\)
</p>

<p>
A is invertible iff \(\det A \ne 0\)
</p>

<p>
\(\det AB = (\det A)(\det B)\)
</p>

<p>
λ is an eigenvalue of A iff \(\det (A-\lambda I) = 0\) (the characteristic equation of A)
</p>

<p>
The eigenvalues of A (n × n) are the solutions for λ. Multiplicity is the number of solutions for λ.
</p>

<div id="Linear Algebra-Similarity"><h2 id="Similarity">Similarity</h2></div>
<p>
given A and B (n × n), A is similar to B if ∃p s.t. \(A = PBP^{-1}\)
</p>

<p>
If A and B are similar, then they have the same characteristic polynomials (and the same eigenvalues with the same multiplicities)
</p>

<div id="Linear Algebra-Diagonalization"><h2 id="Diagonalization">Diagonalization</h2></div>
<p>
A is diagonalizable if A is similar to a diagonal matrix.
</p>

<p>
Diagonalization Theorem: A (n × n) is diagonalizable iff A has n linearly independent eigenvectors (the eigenbasis for \(\Re^n\))
</p>

<p>
\(A = P D P^{-1} \leftrightarrow\) columns of D are linearly independent eigenvectors and the diagonals of D are corresponding eigenvectors.
</p>

<p>
How to diagonalize a matrix:
</p>
<ol>
<li>
Find eigenvalues of A

<li>
Find n = λ linearly independent eigenvectors

<li>
Construct \(P = \begin{bmatrix} p_1 &amp; p_2 &amp; \ldots &amp; p_n \end{bmatrix}\)

<li>
Construct D from the corresponding eigenvalues on the diagonal. Order of eigenvalues must match the order for columns of P.

<li>
Check \(A = p D p^{-1} \leftrightarrow Ap = pD\) (if p is invertible)

</ol>

<p>
If A (n × n) has n distinct eigenvalues, it is diagonalizable.
</p>

<div id="Linear Algebra-Inner (dot) product &amp; uses"><h2 id="Inner (dot) product &amp; uses">Inner (dot) product &amp; uses</h2></div>
<p>
let \(u,v \in \Re^n\). then, \(u \cdot v = u^T v \in \Re\).
</p>

<p>
in English, to calculate you just multiply the vectors row-wise, and sum all the results.
</p>

<p>
Regular algebraic rules apply.
</p>

<p>
\(u \cdot u \geq 0\), only 0 iff u = 0.
</p>

<div id="Linear Algebra-Inner (dot) product &amp; uses-Length of a vector"><h3 id="Length of a vector">Length of a vector</h3></div>
<p>
Let \(v \in \Re^n\), then the norm (length) of v is \(\|v\| = \sqrt{v \cdot v}\).
</p>

<p>
Does the norm coincide with length of line segments? Yes:
</p>

<p>
\(x = \begin{bmatrix}a\\b\end{bmatrix}, \quad \|v\| = \sqrt{v \cdot v} = \sqrt{a^2 + b^2} = \text{Pythagoras}\)
</p>

<div id="Linear Algebra-Inner (dot) product &amp; uses-Distance between vectors"><h3 id="Distance between vectors">Distance between vectors</h3></div>
<p>
Let \(u,v \in \Re^n\). then, \(\text{dist}(u,v) = \|u-v\|\).
</p>

<div id="Linear Algebra-Orthogonality"><h2 id="Orthogonality">Orthogonality</h2></div>
<p>
let \(u,v \in \Re^n\). orthogonal iff:
</p>
<ul>
<li>
\(u \cdot v = 0\)

<li>
or \(\|u\|^2 + \|v\|^2 = \|u+v\|^2\)

</ul>

<div id="Linear Algebra-Orthogonality-Orthogonal complement"><h3 id="Orthogonal complement">Orthogonal complement</h3></div>
<p>
Let \(W \subset \Re^n\) a subspace, then orthogonal complement of W is \(W^\perp = \{x \in \Re^n | x \cdot v = 0 \forall u \in W \}\)
</p>

<p>
properties:
</p>
<ul>
<li>
\((colA)^\perp = (NulA)^T\)

<li>
\((NulA)^\perp = (colA)^T\)

</ul>

<div id="Linear Algebra-Orthogonality-Orthogonal sets"><h3 id="Orthogonal sets">Orthogonal sets</h3></div>
<p>
a set \(\{v_1 \dots v_p\}\) is orthogonal if \(v_i \cdot v_j = 0 \forall i,j\). then \(\{v_1 \dots v_p\} is a basis for \)\text{Span}\{v_1 \dots v_p\}$
</p>

<p>
An orthogonal basis is a basis that is also an orthogonal set
</p>

<p>
Why orthogonal basis? Let \(W \in \Re^n\) be subspace with orthogonal basis \(\{u_1 \dots u_p\}\), then \(W \ni y = c_1 u_1 + \ldots + c_p u_p\), with \(c_i = \frac{y \cdot u_i}{u_i \cdot u_i}\) for i = 1...p.
</p>

<p>
An orthonormal set/basis is an orthogonal set/basis consisting of unit vectors (like \(\{e_1, \ldots, e_n\}\text{ for }\Re^n\)).
</p>

<p>
An m × matrix A has orthonormal columns iff \(A^T A = I_n\)
</p>
<ul>
<li>
\((Ax) \cdot (Ay) = x \cdot y\)

<li>
\(\| Ax \| = \| x \|\)

</ul>

    </div>
</body>
</html>
